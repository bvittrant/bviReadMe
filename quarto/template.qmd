---
title: "Template"
author: "Benjamin Vittrant"
format: 
  html: 
   toc: true
   code-fold: true
   number-sections: true
   number-depth: 2
   anchor-sections: true
   smooth-scroll: true
editor: visual
license: "Copyright Benjamin Vittrant, 2022. All Rights Reserved. The diffusion, reuse or distribution of this document is not allowed."
bibliography: references.bib
---

# Disclaimer

::: callout-important
- This work is only made for internal diffusion
- Ask permission for any image/text/information re-sharing
:::

# Introduction

## Context

## Objectives

## Be aware

::: callout-note
:::

::: callout-warning
:::

::: callout-important
:::

::: callout-tip
:::

::: caution
:::

# Code set-up

## Clean

```{r, message=F, warning=F}
rm(list = ls(all.names = TRUE))
```

## Functions

```{r, message=F, warning=F}
# Install load pkg
install_and_load_packages <- function(pkg_names) {
  for (pkg_name in pkg_names) {
    if (!requireNamespace(pkg_name, quietly = TRUE)) {
      tryCatch({
        if (substr(pkg_name, 1, 2) == "Bi") {
          # Bioconductor package
          BiocManager::install(pkg_name)
          library(pkg_name, character.only = TRUE)
        } else {
          # CRAN package
          install.packages(pkg_name)
          library(pkg_name, character.only = TRUE)
        }
      }, error = function(e) {
        message("Failed to install and load package: ", pkg_name)
      })
    } else {
      library(pkg_name, character.only = TRUE)
    }
  }
}

# Remove duplicated rows
remove_duplicated_rows <- function(df, cols_to_check) {
  # identify the duplicated rows based on the specified columns
  dup_rows <- duplicated(df[, cols_to_check]) | duplicated(df[, cols_to_check], fromLast = TRUE)
  
  # remove the duplicated rows
  df_unique <- df[!dup_rows, ]
  
  # return the updated dataframe
  return(df_unique)
}

# Convert col to character type
convert_cols_to_character <- function(df, colnames_to_convert) {
  # loop through each column name and convert to character type
  for (colname in colnames_to_convert) {
    df[[colname]] <- as.character(df[[colname]])
  }
  
  # return the updated dataframe
  return(df)
}
```

## Packages

```{r, message=F, warning=F}

list_pkg = c("BiocManager","ggsci", "plotly", "GGally", "reshape2","webshot2" ,"gtsummary", "tidyr", "polycor", "rstatix",
             "ComplexHeatmap", "circlize", "cowplot", "dplyr", "reticulate", "renv")
install_and_load_packages(list_pkg)

```

## Env

Initialize a specific R env to work in a isolated env properly. Packages are saved within the renv.lock file.

```{r, message=F, warning=F, eval=F}
renv::init()
```


## Colors

We define color palette from the *ggsci* [@xiao__aut_ggsci_2023] that recreate color from scientific publication.

```{r}
discrete_ggsci_npg = list(scale_color_npg(), scale_fill_npg())
continuous_ggsci_npg = list(scale_fill_material("light-blue"))
palette = "Paired"
```

# Data collection and preprocessing

In this part we will download the data from structured data model *Clickhouse* [@clickhouse_fast_nodate] with a homemade function. 

```{python, message=F, warning=F}
###############################################################################
# Import
###############################################################################

from datetime import datetime
from heapq import merge
from tabnanny import verbose
from clickhouse_driver import Client
import os
import pandas as pd
import numpy as np
import configparser

###############################################################################
# SECURITY
###############################################################################

# !!! WARNINGS !!! #
# File config path - Uniq to each user 
path_config = "/home/bvittrant/Documents/RetD/Python/conf_git/datascience.conf"
configParser = configparser.RawConfigParser()
configParser.read(path_config)


# set path to directory for data and results
Path_to_data = "../data/"
Path_to_data_raw = Path_to_data + "raw/"
Path_to_data_raw_internal = Path_to_data_raw + "internal/"
Path_to_data_processed = Path_to_data + "processed/"

# Headname for all files
headname = "eda_exploration_"

###############################################################################
# FUNCTIONS
###############################################################################

# Check if file exist and if not DL it from the provided query
def download_data_clickhouse(path_to_file, url, query, silence=False):
    
    """
    Download data from your database from your specified query and format it to panda dataframe. 
    
    If the file already exist it just read it isntead of re-downloading it.

    Arguments:
        path_to_file -- Provide the path where to download the data
        url -- Provide the URL connection to your database
        query -- Provide the query you want to send to your database

    Keyword Arguments:
        silence -- No verbose function (default: {False})

    Returns:
        return a panda dataframe with the data from your query
    """

    if os.path.isfile(path_to_file):
        if(silence == False):
            print("File already exists")
        df = pd.read_csv(path_to_file, sep=';')
    else:
        if(silence == False):
            print("I'm connecting & downloading the data from your database")

        # Connect to the client
        client = Client.from_url(url)

        # Download the data from CH client
        data = client.execute_iter(query, with_column_types=True)

        # Build columns name
        columns = [column[0] for column in next(data)]

        # Convert data to pandas DF format
        df = pd.DataFrame.from_records(data, columns=columns)
        
        # save the results
        df.to_csv(path_to_file, index=False, sep=';')

    return df

###############################################################################
# DATABASE CONNECTION
###############################################################################

# Build URL connection for futur sql requests
url = configParser['clickhouse']['ch_name'] + configParser['withings']['USER_LDAP'] \
    + ":" + configParser['withings']['MDP_LDAP'] + configParser['clickhouse']['ch_adress']

###############################################################################
# QUERIES survey data collection
###############################################################################
# Same results but with survey data.
query_xxxx = f'''
'''

# Get & save all the data 
name = Path_to_data_raw_internal + headname +"data_xxxx.csv"
tmp = download_data_clickhouse(name, url, query_xxxx)


###############################################################################
# Create requirement.txt
###############################################################################

os.system("pipreqs --force ../")

###############################################################################
# END
###############################################################################
```


# Feature engineering

```{r, message=F, warning=F}
```

# Exploration

## Distribution

```{r, message=F, warning=F}

```

The distribution can also be checked using a correlogram where feature distribution is the diagonal part. This graph is created from the *GGally* [@schloerke_ggally_2021] library which extend the ggplot 2 capabilities. You need to reduce matrix input or can take a lot of time nad the plot can be unreadable if you have to much features.

```{r, message=F, warning=F}

```

## Summary statistics

For the summary statistics I used the package from *rstatix* [@kassambara_rstatix_2023] function.

```{r, message=F, warning=F}

```

## Correlation

For the correlation analysis I used the pacakge *polycor* [@fox_polycor_2022, @noauthor_polychoric_2023] which is based on polychoric and polyserial correlation [@drasgow_polychoric_2006].

```{r, message=F, warning=F}

```

## Mutual information

```{r, message=F, warning=F}
```

## Heatmap

To plot the heat map I used the unbelieveable package *complex heatmap* [@gu_complex_2016, @gu_complex_2022]. All the doc is availabel on the [github page](https://jokergoo.github.io/ComplexHeatmap-reference/book/index.html).

```{r, message=F, warning=F}
# Select data
#n = 1000
#data_heatmap = data
#data_heatmap_num = data_num
#mat_heatmap = scale(log(data_heatmap_num[1:n,]+1)) %>% as.matrix()

#mat_heatmap = data_num[1:n,] %>% as.matrix()

# Define annotation
#column_ha = HeatmapAnnotation(Barplot = anno_density(mat_heatmap, type = "violin"))
#row_ha = rowAnnotation(Age = data_heatmap[1:n, "age"], Gender = data_heatmap[1:n, "gender"], Continent = data_heatmap[1:n, "continent"], Height = data_heatmap[1:n, "height"])

# Definine color function
#col_fun = colorRamp2(c(-2, 0, 2), hcl_palette = "Tropic")

# Define split
#r_split = data_heatmap.frame(data[1:n, "year"], data[1:n, "continent"])
#r_split = data_heatmap[1:n, "gender"]

# Create the heatmap
#ht = Heatmap(mat_heatmap , show_row_names = F, right_annotation = row_ha, top_annotation = column_ha, col = col_fun,
        row_split = r_split)
#ht
# save the heamap
#pdf("plot_exploration_diabetic_foot_heatmap.pdf", width = 20, height = 15)
#draw(ht)
#dev.off()

```

# Model selection and training

```{r, message=F, warning=F}
```

# Model evaluation

```{r, message=F, warning=F}
```

# Conlcusion

```{r, message=F, warning=F}
```

# Discussion

```{r, message=F, warning=F}
```

# Conclusion/Futur work

```{r, message=F, warning=F}
```

# Thanks

I'd like to thank the scikit-Learn team in general for their amazing work [@scikit-learn;@sklearn_api] and also the *Stack Overflow* [@noauthor_stack_nodate] community and *Github* [@noauthor_github_nodate] for providing code repository.

# References

::: {#refs}
:::
